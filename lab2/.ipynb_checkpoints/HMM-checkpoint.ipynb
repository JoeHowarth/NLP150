{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import ngrams, word_tokenize, RegexpTokenizer, FreqDist, sent_tokenize\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## books liscensces removed\n",
    "#books = [\"SHolmes.txt\",\"Frankenstein.txt\",\"Dracula.txt\", \"TheYellowWallpaper.txt\"]\n",
    "books = [\"Frankenstein.txt\",\"Dracula.txt\", \"TheYellowWallpaper.txt\"]\n",
    "\n",
    "raws = []\n",
    "for book in books:\n",
    "    f = open(book,\"r\")\n",
    "    raws.append(f.read())\n",
    "    f.close()\n",
    "# make lowercase\n",
    "raws = [raw.lower() for raw in raws]\n",
    "\n",
    "# separate into sentences\n",
    "\n",
    "sent_texts = [sent_tokenize(raw) for raw in raws]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize sentence strings to lists of tokens\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'[a-z][a-z\\']*').tokenize\n",
    "texts = []\n",
    "print(len(sent_texts))\n",
    "for i in range(len(sent_texts)):\n",
    "    texts.append( [tokenizer(sent) for sent in sent_texts[i]] )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigramCounts (sents, lexicon):\n",
    "    #len_lex = len(lexicon)\n",
    "    prev = ['start0', 'unk1'] + (lexicon)\n",
    "    curr = ['end1', 'unk1'] + lexicon\n",
    "    \n",
    "    #listBigram = []\n",
    "    \n",
    "   # print(type(prev),type(curr),prev)\n",
    "    counts = pd.DataFrame(0.0, prev, curr)\n",
    "    for sent in sents:\n",
    "        len_sent = len(sent)\n",
    "        \n",
    "        if (len_sent < 2):\n",
    "            continue\n",
    "        \n",
    "        # count start marker\n",
    "        counts.get_value['start0',sent[0]] += 10\n",
    "#         if counts.at['start0',sent[0]] == 1:\n",
    "#             counts.at['start0','unk1'] += .001\n",
    "#             counts.at['unk1', sent[1]] += .001\n",
    "        \n",
    "        for i in range(len_sent - 1):\n",
    "            counts.get_value[sent[i],sent[i+1]] += 10\n",
    "#             if counts.at[sent[i],sent[i+1]] == 1:\n",
    "#                 counts.at[sent[i],'unk1'] += .001\n",
    "#                 if i+2 < len_sent - 1:\n",
    "#                     counts.at['unk1',sent[i+2]] += .001\n",
    "#             \n",
    "            \n",
    "        # for end marker\n",
    "        counts.get_value[sent[len_sent - 1], 'end1'] += 10\n",
    "#         if counts.at[sent[len_sent - 1], 'end1'] == 1:\n",
    "#             counts.at[sent[len_sent - 1], 'unk1'] += .001\n",
    "    return counts\n",
    "\n",
    "def bigramList (sents):\n",
    "    #len_lex = len(lexicon)\n",
    "#     prev = ['start0'] + (lexicon)\n",
    "#     curr = ['end1'] + lexicon\n",
    "    \n",
    "    listBigram = []\n",
    "    \n",
    "   # print(type(prev),type(curr),prev)\n",
    "    for sent in sents:\n",
    "        len_sent = len(sent)\n",
    "        if (len_sent == 0):\n",
    "            continue\n",
    "        \n",
    "        # count start marker\n",
    "        listBigram.append(('start0',sent[0]))\n",
    "        \n",
    "        for i in range(len_sent - 1):\n",
    "            listBigram.append((sent[i],sent[i+1]))\n",
    "            \n",
    "        # for end marker\n",
    "        listBigram.append((sent[len_sent - 2],'end1'))\n",
    "    return listBigram\n",
    "\n",
    "def trigramList (sents):\n",
    "    #len_lex = len(lexicon)\n",
    "#     prev = ['start0'] + (lexicon)\n",
    "#     curr = ['end1'] + lexicon\n",
    "    \n",
    "    listBigram = []\n",
    "    \n",
    "   # print(type(prev),type(curr),prev)\n",
    "    for sent in sents:\n",
    "        len_sent = len(sent)\n",
    "        if (len_sent < 2):\n",
    "            continue\n",
    "        \n",
    "        # count start marker\n",
    "        listBigram.append(('start0',sent[0],sent[1]))\n",
    "        \n",
    "        for i in range(len_sent - 2):\n",
    "            listBigram.append((sent[i],sent[i+1],sent[i+2]))\n",
    "            \n",
    "        # for end marker\n",
    "        listBigram.append((sent[len_sent - 3], sent[len_sent - 2],'end1'))\n",
    "    return listBigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "book_lexicon = []\n",
    "for i in range(len(texts)):\n",
    "    flat = [word for sent in texts[i] for word in sent]\n",
    "    book_lexicon.append(sorted(set(flat)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def freq_bigrams(sents, lexicon, min_num):\n",
    "    listBigram = bigramList(sents)\n",
    "    fdist = FreqDist(listBigram)\n",
    "    big_bgram = [ k for k,v in fdist.items() if v>min_num]\n",
    "    return big_bgram\n",
    "\n",
    "def freq_trigrams(sents, lexicon, min_num):\n",
    "    listBigram = trigramList(sents)\n",
    "    fdist = FreqDist(listBigram)\n",
    "    big_bgram = [ k for k,v in fdist.items() if v>min_num]\n",
    "    return big_bgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frank_common = freq_trigrams(texts[0] ,book_lexicon[0], 6)\n",
    "drac_common = freq_trigrams(texts[1] ,book_lexicon[1], 6)\n",
    "yellow_common = freq_trigrams(texts[2] ,book_lexicon[2], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(frank_common),len(drac_common),len(yellow_common))\n",
    "\n",
    "FnD = set(frank_common).intersection(drac_common)\n",
    "FnY = set(frank_common).intersection(yellow_common)\n",
    "DnY = set(drac_common).intersection(yellow_common)\n",
    "\n",
    "print(len(FnD),len(FnY),len(DnY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = [bigramCounts(texts[i], book_lexicon[i]) for i in range(3)]\n",
    "counts_smooth = [count.add(0.001) for count in counts]\n",
    "\n",
    "#frank_counts = bigramCounts(texts[0], book_lexicon[0])\n",
    "#frank_counts_smooth = frank_counts.add(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_smooth[0].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = [count.div(count.sum(axis=1), axis=0) for count in counts_smooth]\n",
    "#prob = frank_counts_smooth.div(frank_counts_smooth.sum(axis=1), axis=0)\n",
    "counts_smooth[0].sum(axis=1).sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_word(prob, prev_word):\n",
    "    P = prob.loc[prev_word,:].as_matrix()\n",
    "    x = random()\n",
    "    for i in range(len(P)):\n",
    "        x -= P[i]\n",
    "        if x <= 0:\n",
    "            word = list(prob)[i]\n",
    "            break;\n",
    "    return word\n",
    "\n",
    "def make_sentence(prob):\n",
    "    sentence = \"\"\n",
    "    word = choose_word(prob, \"start0\")\n",
    "    \n",
    "    while (word != \"end1\"):\n",
    "        sentence += word + \" \"\n",
    "        word = choose_word(prob, word)\n",
    "    return sentence\n",
    "\n",
    "def clean(prob, sent):\n",
    "    for i in range(len(sent)):\n",
    "        if sent[i] not in prob:\n",
    "            sent[i] = 'unk1'\n",
    "    return sent\n",
    "    \n",
    "def prob_sentence(prob, sent):\n",
    "    sent = clean(set(prob), sent)\n",
    "    P = 1\n",
    "    if len(sent) < 2:\n",
    "        return 1\n",
    "    \n",
    "    P *= prob.at['start0',sent[0]]\n",
    "    \n",
    "    len_sent = len(sent)\n",
    "    for i in range(min(len_sent - 1, 5)):\n",
    "        P *= prob.at[sent[i],sent[i+1]]\n",
    "    #P *= prob.at[sent[len_sent - 1], 'end1']\n",
    "    return P\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted([(choose_word(prob, \"all\")) for x in range(100)])\n",
    "make_sentence(probs[0])\n",
    "make_sentence(probs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prob_sentence(probs[0], texts[0][80]))\n",
    "print(prob_sentence(probs[0], texts[2][83]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = [prob_sentence(probs[0], sents) for sents in texts[0]]\n",
    "xy = [prob_sentence(probs[0], sents) for sents in texts[1]]\n",
    "xz = [prob_sentence(probs[0], sents) for sents in texts[2]]\n",
    "\n",
    "yx = [prob_sentence(probs[1], sents) for sents in texts[0]]\n",
    "yy = [prob_sentence(probs[1], sents) for sents in texts[1]]\n",
    "yz = [prob_sentence(probs[1], sents) for sents in texts[2]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zx = [prob_sentence(probs[2], sents) for sents in texts[0]]\n",
    "zy = [prob_sentence(probs[2], sents) for sents in texts[1]]\n",
    "zz = [prob_sentence(probs[2], sents) for sents in texts[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(xx), np.mean(xy), np.mean(xz))\n",
    "print(np.mean(yx), np.mean(yy), np.mean(yz))\n",
    "print(np.mean(zx), np.mean(zy), np.mean(zz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean([len(sent) for text in texts for sent in text ]))\n",
    "for text in texts:\n",
    "    print(np.mean([len(sent) for sent in text ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(probs[0].at['by','sat'])\n",
    "print(probs[1].at['by','sat'])\n",
    "print(probs[2].at['by','sat'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs[0].sum(axis=0).sort_values(ascending=False)\n",
    "\n",
    "pd.DataFrame(random(), book_lexicon[2],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
