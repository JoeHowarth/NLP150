{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import ngrams, word_tokenize, RegexpTokenizer, FreqDist, sent_tokenize\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     5
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "## books liscensces removed\n",
    "books = [\"SHolmes.txt\",\"Frankenstein.txt\",\"Dracula.txt\", \"TheYellowWallpaper.txt\"]\n",
    "#books = [\"Frankenstein.txt\",\"Dracula.txt\", \"TheYellowWallpaper.txt\"]\n",
    "#books = [\"SHolmes.txt\",\"Frankenstein.txt\",\"Dracula.txt\"]\n",
    "\n",
    "raws = []\n",
    "for book in books:\n",
    "    f = open(book,\"r\")\n",
    "    raws.append(f.read())\n",
    "    f.close()\n",
    "## make lowercase\n",
    "raws = [raw.lower() for raw in raws]\n",
    "\n",
    "## separate into sentences\n",
    "\n",
    "sent_texts = [sent_tokenize(raw) for raw in raws]\n",
    "\n",
    "## tokenize sentence strings to lists of tokens\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'[a-z][a-z\\']*').tokenize\n",
    "texts = []\n",
    "print(len(sent_texts))\n",
    "\n",
    "for i in range(len(sent_texts)):\n",
    "    texts.append( [tokenizer(sent) for sent in sent_texts[i]] )\n",
    "    \n",
    "## make lexicon\n",
    "book_lexicon = []\n",
    "for i in range(len(texts)):\n",
    "    flat = [word for sent in texts[i] for word in sent]\n",
    "    book_lexicon.append(sorted(set(flat)))\n",
    "\n",
    "## Tokens per book \n",
    "flats = [[T for s in text for T in s] for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def bigramCounts (sents, lexicon):\n",
    "    prev = ['start0', 'unk1'] + (lexicon)\n",
    "    curr = ['end1', 'unk1'] + lexicon    \n",
    "\n",
    "    counts = pd.DataFrame(0.0, prev, curr)\n",
    "    for sent in sents:\n",
    "        len_sent = len(sent)\n",
    "        \n",
    "        if (len_sent < 2):\n",
    "            continue\n",
    "        \n",
    "        # count start marker\n",
    "        counts.at['start0',sent[0]] += 10\n",
    "      \n",
    "        for i in range(len_sent - 1):\n",
    "            counts.at[sent[i],sent[i+1]] += 1 \n",
    "            \n",
    "        # for end marker\n",
    "        counts.at[sent[len_sent - 1], 'end1'] += 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     1,
     36
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## ngram lists and freqs\n",
    "def bigramList (sents):\n",
    "    listBigram = []\n",
    "    \n",
    "    for sent in sents:\n",
    "        len_sent = len(sent)\n",
    "        if (len_sent == 0):\n",
    "            continue\n",
    "        \n",
    "        # count start marker\n",
    "        listBigram.append(('start0',sent[0]))\n",
    "        \n",
    "        for i in range(len_sent - 1):\n",
    "            listBigram.append((sent[i],sent[i+1]))\n",
    "            \n",
    "        # for end marker\n",
    "        listBigram.append((sent[len_sent - 2],'end1'))\n",
    "    return listBigram\n",
    "def trigramList (sents):\n",
    "    \n",
    "    listBigram = []\n",
    "    \n",
    "    for sent in sents:\n",
    "        len_sent = len(sent)\n",
    "        if (len_sent < 2):\n",
    "            continue\n",
    "        \n",
    "        # count start marker\n",
    "        listBigram.append(('start0',sent[0],sent[1]))\n",
    "        \n",
    "        for i in range(len_sent - 2):\n",
    "            listBigram.append((sent[i],sent[i+1],sent[i+2]))\n",
    "            \n",
    "        # for end marker\n",
    "        listBigram.append((sent[len_sent - 3], sent[len_sent - 2],'end1'))\n",
    "    return listBigram\n",
    "def freq_bigrams(sents, min_num):\n",
    "    listBigram = bigramList(sents)\n",
    "    fdist = FreqDist(listBigram)\n",
    "    big_bgram = [ k for k,v in fdist.items() if v>min_num]\n",
    "    return big_bgram\n",
    "def freq_trigrams(sents, min_num):\n",
    "    listBigram = trigramList(sents)\n",
    "    fdist = FreqDist(listBigram)\n",
    "    big_bgram = [ k for k,v in fdist.items() if v>min_num]\n",
    "    return big_bgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     2
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### similarity measures\n",
    "## JacardSimilarity\n",
    "def jaccard_similarity (doc1, doc2):\n",
    "    s1 = set(doc1)\n",
    "    s2 = set(doc2)\n",
    "    inter = s1.intersection(s2)\n",
    "    union = s1.union(s2)\n",
    "    return len(inter) / len(union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### find cosine similarity with ngrams\n",
    "## make list of trigrams for each book\n",
    "## FreqDist list for each book and combined list\n",
    "    # Term frequency calculated \n",
    "## calculate inverse document frequency from combined list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sublinear_tf (fdist):\n",
    "    return {k: (1+ math.log(v)) for k,v in fdist.items()}\n",
    "\n",
    "def inv_doc_freq (ngs):\n",
    "    idfs = {}\n",
    "    ## make lexicon of n-grams for each book\n",
    "    lexs = [set(ngL) for ngL in ngs]\n",
    "    num_books = len(ngs)\n",
    "    lex_comb = set().union(*lexs)\n",
    "    for ngram in lex_comb:\n",
    "        num_matches = sum([ngram in lex for lex in lexs])\n",
    "        idfs[ngram] = 1 + math.log(num_books / num_matches )\n",
    "    #print(idfs.keys())\n",
    "    return idfs\n",
    "\n",
    "def tfidf(ngs):\n",
    "    idfs = inv_doc_freq(ngs)\n",
    "    tfidf_books = []\n",
    "      \n",
    "    for ngL in ngs:\n",
    "        book_tfidf = []\n",
    "        fdist = FreqDist(ngL)\n",
    "        tfDict = sublinear_tf(fdist)\n",
    "        \n",
    "        for term in idfs.keys():\n",
    "            tf = tfDict.get(term, 0)\n",
    "            book_tfidf.append(tf * idfs.get(term, 0))\n",
    "            \n",
    "        tfidf_books.append(book_tfidf)\n",
    "    return tfidf_books\n",
    "\n",
    "def cosine_similarity(vector1, vector2):\n",
    "    dot_product = sum(p*q for p,q in zip(vector1, vector2))\n",
    "    magnitude = math.sqrt(sum([val**2 for val in vector1])) * math.sqrt(sum([val**2 for val in vector2]))\n",
    "    if not magnitude:\n",
    "        return 0\n",
    "    return dot_product/magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.245, 'SHolmes.txt', 'Dracula.txt')\n",
      "(0.1804, 'Frankenstein.txt', 'Dracula.txt')\n",
      "(0.1705, 'SHolmes.txt', 'Frankenstein.txt')\n",
      "(0.1187, 'Dracula.txt', 'TheYellowWallpaper.txt')\n",
      "(0.1111, 'SHolmes.txt', 'TheYellowWallpaper.txt')\n",
      "(0.0803, 'Frankenstein.txt', 'TheYellowWallpaper.txt')\n"
     ]
    }
   ],
   "source": [
    "ngs = [bigramList(text) for text in texts]\n",
    "rep_tfidf = tfidf(ngs)\n",
    "means = [np.mean(x) for x in rep_tfidf]\n",
    "#[print(mean) for mean in means]\n",
    "\n",
    "tfidf_comparisons = []\n",
    "for i, doc_0 in enumerate(rep_tfidf):\n",
    "    for j, doc_1 in enumerate(rep_tfidf):\n",
    "        if (j >= i):\n",
    "            tfidf_comparisons.append((round(cosine_similarity(doc_0, doc_1),4), books[i], books[j]))\n",
    "\n",
    "for x in sorted(tfidf_comparisons, reverse=True):\n",
    "    if (x[0] != 1.0):\n",
    "        print(x)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf(texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = [bigramCounts(texts[i], book_lexicon[i]) for i in range(3)]\n",
    "counts_smooth = [count.add(0.001) for count in counts]\n",
    "\n",
    "#frank_counts = bigramCounts(texts[0], book_lexicon[0])\n",
    "#frank_counts_smooth = frank_counts.add(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counts_smooth[0].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probs = [count.div(count.sum(axis=1), axis=0) for count in counts_smooth]\n",
    "#prob = frank_counts_smooth.div(frank_counts_smooth.sum(axis=1), axis=0)\n",
    "counts_smooth[0].sum(axis=1).sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     10
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def choose_word(prob, prev_word):\n",
    "    P = prob.loc[prev_word,:].as_matrix()\n",
    "    x = random()\n",
    "    for i in range(len(P)):\n",
    "        x -= P[i]\n",
    "        if x <= 0:\n",
    "            word = list(prob)[i]\n",
    "            break;\n",
    "    return word\n",
    "\n",
    "def make_sentence(prob):\n",
    "    sentence = \"\"\n",
    "    word = choose_word(prob, \"start0\")\n",
    "    \n",
    "    while (word != \"end1\"):\n",
    "        sentence += word + \" \"\n",
    "        word = choose_word(prob, word)\n",
    "    return sentence\n",
    "\n",
    "def clean(prob, sent):\n",
    "    for i in range(len(sent)):\n",
    "        if sent[i] not in prob:\n",
    "            sent[i] = 'unk1'\n",
    "    return sent\n",
    "    \n",
    "def prob_sentence(prob, sent):\n",
    "    sent = clean(set(prob), sent)\n",
    "    P = 1\n",
    "    if len(sent) < 2:\n",
    "        return 1\n",
    "    \n",
    "    P *= prob.at['start0',sent[0]]\n",
    "    \n",
    "    len_sent = len(sent)\n",
    "    for i in range(min(len_sent - 1, 5)):\n",
    "        P *= prob.at[sent[i],sent[i+1]]\n",
    "    #P *= prob.at[sent[len_sent - 1], 'end1']\n",
    "    return P\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted([(choose_word(prob, \"all\")) for x in range(100)])\n",
    "make_sentence(probs[0])\n",
    "make_sentence(probs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(prob_sentence(probs[0], texts[0][80]))\n",
    "print(prob_sentence(probs[0], texts[2][83]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## all probs\n",
    "xx = [prob_sentence(probs[0], sents) for sents in texts[0]]\n",
    "xy = [prob_sentence(probs[0], sents) for sents in texts[1]]\n",
    "xz = [prob_sentence(probs[0], sents) for sents in texts[2]]\n",
    "\n",
    "yx = [prob_sentence(probs[1], sents) for sents in texts[0]]\n",
    "yy = [prob_sentence(probs[1], sents) for sents in texts[1]]\n",
    "yz = [prob_sentence(probs[1], sents) for sents in texts[2]]\n",
    "\n",
    "zx = [prob_sentence(probs[2], sents) for sents in texts[0]]\n",
    "zy = [prob_sentence(probs[2], sents) for sents in texts[1]]\n",
    "zz = [prob_sentence(probs[2], sents) for sents in texts[2]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    \n",
    "a_b = jaccard_similarity(flats[0],flats[1])\n",
    "b_c = jaccard_similarity(flats[1],flats[2])\n",
    "a_c = jaccard_similarity(flats[0],flats[2])\n",
    "print(round(a_b, 3), round(b_c, 3), round(a_c, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(np.mean(xx), np.mean(xy), np.mean(xz))\n",
    "print(np.mean(yx), np.mean(yy), np.mean(yz))\n",
    "print(np.mean(zx), np.mean(zy), np.mean(zz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(np.mean([len(sent) for text in texts for sent in text ]))\n",
    "for text in texts:\n",
    "    print(np.mean([len(sent) for sent in text ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(probs[0].at['by','sat'])\n",
    "print(probs[1].at['by','sat'])\n",
    "print(probs[2].at['by','sat'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probs[0].sum(axis=0).sort_values(ascending=False)\n",
    "\n",
    "pd.DataFrame(random(), book_lexicon[2],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frank_common = freq_trigrams(texts[0] ,book_lexicon[0], 6)\n",
    "drac_common = freq_trigrams(texts[1] ,book_lexicon[1], 6)\n",
    "yellow_common = freq_trigrams(texts[2] ,book_lexicon[2], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(frank_common),len(drac_common),len(yellow_common))\n",
    "\n",
    "FnD = set(frank_common).intersection(drac_common)\n",
    "FnY = set(frank_common).intersection(yellow_common)\n",
    "DnY = set(drac_common).intersection(yellow_common)\n",
    "\n",
    "print(len(FnD),len(FnY),len(DnY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
